import os
import requests
import pdfplumber
import time
import random
import hashlib
import json
from threading import Lock
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# === CONFIGURATION ===

# Provide Gemini API keys here or via env var "GEMINI_API_KEYS" comma-separated.
API_KEYS = []
env_keys = os.getenv("GEMINI_API_KEYS")
if env_keys:
    API_KEYS = [k.strip() for k in env_keys.split(",") if k.strip()]
if not API_KEYS:
    # Replace with your actual keys if hardcoding; you can have 2-3 to spread load.
    API_KEYS = [
        "AIzaSyAUJWlANalizPlnIJkhYQuRWrz6uaHo8kk",  
        "AIzaSyBPqm-D1_t91buGCC357teibBTjpIDeL5A"
        "AIzaSyBnTnGAdqm7IHI-II0ml7UqlaQPTXcd_Ig",
    ]

MODEL = os.getenv("GEMINI_MODEL", "models/gemini-2.0-flash")
ENDPOINT_TEMPLATE = "https://generativelanguage.googleapis.com/v1beta/{model}:generateContent"

CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 9000))
MAX_CALLS_PER_MINUTE_PER_KEY = 15  # free-tier limit; pacing implemented

# Cache for (question+context) -> answer to avoid repeated LLM calls
CACHE_PATH = os.getenv("ANSWER_CACHE_PATH", "answer_cache.json")
_cache_lock = Lock()

# Rate window tracking per key
_rate_windows = {key: [] for key in API_KEYS}
_rate_lock = Lock()


def _load_cache():
    try:
        with open(CACHE_PATH, "r") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_cache(cache):
    try:
        with open(CACHE_PATH, "w") as f:
            json.dump(cache, f)
    except Exception:
        pass


def _cache_key(prefix, content):
    h = hashlib.sha256()
    snippet = content if len(content) < 5000 else content[:5000]
    h.update((prefix + "\n" + snippet).encode("utf-8"))
    return h.hexdigest()


def extract_text_chunks(pdf_path, chunk_size=CHUNK_SIZE):
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                full_text += t + "\n"
    chunks = [full_text[i : i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    return chunks, full_text


def build_tfidf_index(chunks):
    vectorizer = TfidfVectorizer().fit(chunks)
    matrix = vectorizer.transform(chunks)
    return vectorizer, matrix


def top_k_chunks(question, vectorizer, matrix, chunks, k=3):
    q_vec = vectorizer.transform([question])
    sims = cosine_similarity(q_vec, matrix).flatten()
    top_indices = sims.argsort()[::-1][:k]
    selected = []
    for idx in top_indices:
        if sims[idx] > 0:
            selected.append(chunks[idx])
    return selected if selected else chunks[:1]


def _wait_for_slot_for_key(key):
    with _rate_lock:
        now = time.monotonic()
        window = _rate_windows.setdefault(key, [])
        # purge old timestamps
        while window and now - window[0] > 60:
            window.pop(0)
        if len(window) >= MAX_CALLS_PER_MINUTE_PER_KEY:
            to_sleep = 60 - (now - window[0]) + 0.01
            time.sleep(to_sleep)
            now = time.monotonic()
            while window and now - window[0] > 60:
                window.pop(0)
        window.append(now)


def call_gemini_with_prompt(prompt, max_attempts=6):
    endpoint = ENDPOINT_TEMPLATE.format(model=MODEL)
    cache = _load_cache()
    cache_key = _cache_key("prompt", prompt)
    if cache_key in cache:
        return cache[cache_key]

    attempt = 0
    key_index = 0
    last_exc = None
    while attempt < max_attempts:
        api_key = API_KEYS[key_index % len(API_KEYS)]
        try:
            _wait_for_slot_for_key(api_key)
            headers = {
                "Content-Type": "application/json",
                "X-goog-api-key": api_key,
            }
            body = {
                "contents": [
                    {
                        "parts": [
                            {"text": prompt}
                        ]
                    }
                ]
            }
            resp = requests.post(endpoint, headers=headers, json=body, timeout=30)
            if resp.ok:
                data = resp.json()
                if not data.get("candidates"):
                    raise Exception("No answer generated by Gemini API.")
                answer = data["candidates"][0]["content"]["parts"][0]["text"].strip()
                with _cache_lock:
                    cache[cache_key] = answer
                    _save_cache(cache)
                return answer
            elif resp.status_code == 429:
                # Rate limited: exponential backoff and rotate key
                retry_after = None
                if "Retry-After" in resp.headers:
                    try:
                        retry_after = float(resp.headers["Retry-After"])
                    except:
                        pass
                sleep = retry_after if retry_after else min(2 ** attempt, 10) + random.uniform(0, 0.2)
                time.sleep(sleep)
                attempt += 1
                key_index += 1
                last_exc = Exception(f"Rate limited on key {api_key}: {resp.text}")
                continue
            elif 500 <= resp.status_code < 600:
                time.sleep(min(2 ** attempt, 10))
                attempt += 1
                last_exc = Exception(f"Server error {resp.status_code}: {resp.text}")
                continue
            else:
                raise Exception(f"Gemini API Error: {resp.status_code} {resp.text}")
        except requests.RequestException as e:
            time.sleep(min(2 ** attempt, 5))
            attempt += 1
            key_index += 1
            last_exc = e
            continue

    raise last_exc or Exception("Gemini failed after retries.")


def ask_gemini(question, context):
    # Primary answer prompt: ask concisely, limit to 2-3 sentences
    prompt = (
        "You are an expert at extracting precise answers from policy/contract documents. "
        "Given the document content below and the question, answer clearly and concisely in no more than two sentences. "
        "If the answer is present, summarize it. If not present, reply exactly: Not specified in the document.\n\n"
        f"Document Content:\n{context}\n\n"
        f"Question: {question}\n\n"
        "Answer:"
    )
    return call_gemini_with_prompt(prompt)


def condense_answer(answer_text):
    # If answer is already short, skip
    if len(answer_text.split()) <= 70:
        return answer_text
    # Second-pass to make it even more concise
    prompt = (
        "Take the following answer and rewrite it to be as concise as possible, preserving all key facts, "
        "in one or two sentences. Answer:\n\n"
        f"{answer_text}\n\n"
        "Concise Answer:"
    )
    return call_gemini_with_prompt(prompt)


def answer_questions_from_pdf(pdf_path, questions):
    chunks, full_text = extract_text_chunks(pdf_path)
    vectorizer, matrix = build_tfidf_index(chunks)
    answers = []

    for q in questions:
        # Retrieve relevant context
        selected_chunks = top_k_chunks(q, vectorizer, matrix, chunks, k=3)
        context = "\n---\n".join(selected_chunks)
        if len(context) > CHUNK_SIZE:
            context = context[:CHUNK_SIZE]

        try:
            raw = ask_gemini(q, context)
        except Exception as e:
            answers.append(f"Error: {e}")
            continue

        # Fallback: if "Not specified" try full text once
        if raw.strip().lower().startswith("not specified"):
            try:
                raw_full = ask_gemini(q, full_text[:CHUNK_SIZE])
                if not raw_full.strip().lower().startswith("not specified"):
                    raw = raw_full
                # else keep original
            except Exception:
                pass

        # Optional condensation to standardize brevity
        try:
            concise = condense_answer(raw)
        except Exception:
            concise = raw  # on failure fallback

        answers.append(concise.strip())

    return answers
